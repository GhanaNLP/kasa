{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/epfml/sent2vec\r\n",
      "  Cloning https://github.com/epfml/sent2vec to /tmp/pip-req-build-_75a1j6t\r\n",
      "  Running command git clone -q https://github.com/epfml/sent2vec /tmp/pip-req-build-_75a1j6t\r\n",
      "Building wheels for collected packages: sent2vec\r\n",
      "  Building wheel for sent2vec (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Created wheel for sent2vec: filename=sent2vec-0.0.0-cp36-cp36m-linux_x86_64.whl size=1137474 sha256=12703f152ae23c528b3cee90794587b2ca576b30b34b54dd0638a5b3dcb3864f\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-gbc30ioy/wheels/7b/db/9d/db816db406f182ce88bf1b90e6d41313bf16c7ed72a9626ac7\r\n",
      "Successfully built sent2vec\r\n",
      "Installing collected packages: sent2vec\r\n",
      "Successfully installed sent2vec-0.0.0\r\n",
      "Requirement already satisfied: annoy in /opt/conda/lib/python3.6/site-packages (1.16.3)\r\n"
     ]
    }
   ],
   "source": [
    "# install sent2vec\n",
    "!pip install git+https://github.com/epfml/sent2vec\n",
    "# install annoy\n",
    "!pip install annoy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write requirements to file, anytime you run it, in case you have to go back and recover dependencies.\n",
    "\n",
    "Latest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https://github.com/azunre/transfer-learning-for-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > kaggle_image_requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what is in the input folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jw300entw  sent2vec\r\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../input/\" directory.\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "!ls ../input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the english sentences took 5 seconds\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "english_sentences = []\n",
    "with open(\"../input/jw300entw/jw300.en-tw.en\") as f:\n",
    "    for line in f:\n",
    "        english_sentences.append(re.sub(r'[\\W\\d]', \" \",line.lower())) # clean and normalize\n",
    "end = time.time()\n",
    "print(\"Loading the english sentences took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the english sentences is:\n",
      "['  oh   jehovah   keep my young girl faithful     ', 'i was born in      in alsace   france   into an artistic family   ', 'during the evenings   father   sitting in his lounge chair   would be reading some books about geography or astronomy   ', 'my doggy would be sleeping by his feet   and daddy would be sharing with mum some highlights from his reading while she was knitting for her family   ', 'how much i enjoyed those evenings   ', 'religion played a big part in our lives   ', 'we were staunch catholics   and people who saw us going to church on sunday morning would say     it s nine o clock   ', 'the arnolds are going to church     ', 'every day before going to school   i went to church   ', 'but because of the priest s misbehavior   mum forbade me to go to church alone   ']\n",
      "The length of the list is:\n",
      "606197\n"
     ]
    }
   ],
   "source": [
    "print(\"A sample of the english sentences is:\")\n",
    "print(english_sentences[:10])\n",
    "print(\"The length of the list is:\")\n",
    "print(len(english_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "twi_sentences = []\n",
    "with open(\"../input/jw300entw/jw300.en-tw.tw\") as f:\n",
    "    for line in f:\n",
    "        twi_sentences.append(re.sub(r'[\\W\\d]', \" \", line.lower())) # clean and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sample of the twi sentences is:\n",
      "['  oo   yehowa   boa me babea kumaa yi ma onni nokware     ', 'wɔwoo me too abusua a wonim adwinne di mu wɔ alsace   france   wɔ      mu   ', 'ná papa taa pa twere n agua mu kenkan asase ho nsɛm anaa ewim nneɛma ho nhoma bi anwummere anwummere   ', 'ná me kraman da papa nan so   na na ɔka nsɛntitiriw a epue wɔ n akenkan no mu kyerɛ maame bere a ɔnwene abusua no nneɛma no   ', 'ná m ani gye anwummere a ɛtete saa no ho kɛse   ', 'ná yɛpɛ nyamesom kɛse   ', 'ná yɛyɛ katolekfo amapa   na na nkurɔfo a wohu yɛn sɛ yɛrekɔ asɔre kwasida anɔpa no ka sɛ     abɔ nɔnkron   ', 'arnold abusua no rekɔ asɔre     ', 'ná mekɔ asɔre anɔpa biara ansa na makɔ sukuu   ', 'nanso esiane ɔsɔfo no suban bɔne nti   maame amma me nkutoo ankɔ asɔre bio   ']\n",
      "The length of the list is:\n",
      "606197\n"
     ]
    }
   ],
   "source": [
    "print(\"A sample of the twi sentences is:\")\n",
    "print(twi_sentences[:10])\n",
    "print(\"The length of the list is:\")\n",
    "print(len(twi_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize Subset of English Sentences with sent2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXNSENT = 10000 # how many sentences to take from top of data for now (small experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the sent2vec embedding took 10 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sent2vec\n",
    "\n",
    "model = sent2vec.Sent2vecModel()\n",
    "start=time.time()\n",
    "model.load_model('../input/sent2vec/wiki_unigrams.bin')\n",
    "end = time.time()\n",
    "print(\"Loading the sent2vec embedding took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_embedding_vectors(data):\n",
    "    out = None\n",
    "    for item in data:\n",
    "        vec = model.embed_sentence(item)\n",
    "        if vec is not None:\n",
    "            if out is not None:\n",
    "                out = np.concatenate((out,vec),axis=0)\n",
    "            else:\n",
    "                out = vec                                            \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing all embeddings took 31 seconds\n",
      "[[ 1.18584879e-01 -2.13146985e-01 -3.34718257e-01 ...  3.40525210e-01\n",
      "   2.87434489e-01 -1.28532574e-01]\n",
      " [ 2.40930080e-01 -4.59281594e-01 -3.28957111e-01 ...  3.89363542e-02\n",
      "   3.97189409e-01 -3.97409573e-02]\n",
      " [-2.81732213e-02 -6.40245825e-02  3.14420313e-01 ... -4.41329218e-02\n",
      "  -2.96688620e-02  3.62253864e-04]\n",
      " ...\n",
      " [-5.24406433e-02  4.13849652e-01 -1.14870206e-01 ...  7.23682165e-01\n",
      "  -2.16702804e-01 -2.00715717e-02]\n",
      " [ 2.01901551e-02 -3.09839964e-01 -1.10622898e-01 ... -4.58649695e-02\n",
      "   8.25465545e-02  1.08406030e-01]\n",
      " [ 7.83808351e-01  1.92601413e-01  5.50085455e-02 ...  1.12677053e-01\n",
      "   3.22284132e-01  3.28299969e-01]]\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "EmbeddingVectors = assemble_embedding_vectors(english_sentences[:MAXNSENT])\n",
    "end = time.time()\n",
    "print(\"Computing all embeddings took %d seconds\"%(end-start))\n",
    "print(EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of embedding matrix:\n",
      "(10000, 600)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of embedding matrix:\")\n",
    "print(EmbeddingVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for later use\n",
    "np.save(\"english_sent2vec_vectors_jw.npy\",EmbeddingVectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Test Index w/ Annoy for fast Neareast-Neighbor Retrieval\n",
    "\n",
    "First build the annoy index for the available English sent2vec vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the NN index took 1 seconds\n"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "start = time.time()\n",
    "dimension = EmbeddingVectors.shape[1] # Length of item vector that will be indexed\n",
    "english_NN_index = AnnoyIndex(dimension, 'angular')  \n",
    "for i in range(EmbeddingVectors.shape[0]): # go through every embedding vector\n",
    "    english_NN_index.add_item(i, EmbeddingVectors[i]) # add to index\n",
    "\n",
    "english_NN_index.build(10) # 10 trees\n",
    "english_NN_index.save('en_sent2vec_NN_index.ann') # save index\n",
    "end = time.time()\n",
    "print(\"Building the NN index took %d seconds\"%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the built index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_english_NN_index = AnnoyIndex(dimension, 'angular')\n",
    "test_english_NN_index.load('en_sent2vec_NN_index.ann') # super fast, will just mmap the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_idx = 15 # choose index of sentence to focus on in english_sentences/twi_sentences\n",
    "\n",
    "annoy_out = test_english_NN_index.get_nns_by_item(translation_idx, 5) # will 5 nearest neighbors to the very first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 8659, 9452, 1636, 4009]\n"
     ]
    }
   ],
   "source": [
    "print(annoy_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- The sentence we are finding nearest neighbors of:\n",
      "\n",
      "but mother was so enthusiastic about the truth that she decided to do some bible reading with me   \n",
      "\n",
      "\n",
      "- The 4 nearest neighbors found:\n",
      "\n",
      "1. after extended discussions mother too became convinced that what she had learned from the bible was the truth   \n",
      "2. he also spoke to me about maria stossier   our neighbor hans   younger sister   who had taken a stand for bible truth   \n",
      "3.   however   we feel that the real encouragement was for the rest of us who were able to meet them and witness their love and zeal for bible truth     \n",
      "4. my grandfather listened to the same lecture   and he too was convinced that what he heard was the truth   \n"
     ]
    }
   ],
   "source": [
    "print(\"- The sentence we are finding nearest neighbors of:\\n\")\n",
    "print(english_sentences[annoy_out[0]])\n",
    "print(\"\\n\\n- The 4 nearest neighbors found:\\n\")\n",
    "for i in range(1,5):\n",
    "    print(str(i) + \". \"+ english_sentences[annoy_out[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- In other words, if we were translating the english sentence:\n",
      "\n",
      "but mother was so enthusiastic about the truth that she decided to do some bible reading with me   \n",
      "  where the known correct translation is:\n",
      "nanso na maame ani gye nokware no ho araa ma ɔne me boom kenkan bible no   \n",
      "\n",
      "\n",
      "- The 4 top translation suggested by our sparse retrieval system above are:\n",
      "\n",
      "1. bere a wɔne maame bɔɔ nkɔmmɔ pii akyi no   ɔno nso begye dii sɛ nea na wasua afi bible mu no ne nokware no   \n",
      "2. ɔkaa maria stossier   yɛn fipamfo hans nuabea kumaa a na wagyina bible mu nokware akyi   ho asɛm kyerɛɛ me   \n",
      "3. nanso   yɛte nka sɛ yɛn a yɛaka a yenyaa hokwan hyiaa wɔn na yehuu wɔn dɔ ne wɔn mmɔdenbɔ wɔ bible mu nokware ho no na yɛanya nkuranhyɛ kɛse     \n",
      "4. me nana barima tiee saa ɔkasa no bi   na ɔno nso begye dii sɛ nokware no na wate no   \n"
     ]
    }
   ],
   "source": [
    "print(\"- In other words, if we were translating the english sentence:\\n\")\n",
    "print(english_sentences[annoy_out[0]])\n",
    "print(\"  where the known correct translation is:\")\n",
    "print(twi_sentences[annoy_out[0]])\n",
    "print(\"\\n\\n- The 4 top translation suggested by our sparse retrieval system above are:\\n\")\n",
    "for i in range(1,5):\n",
    "    print(str(i) + \". \"+ twi_sentences[annoy_out[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to work! Now we need to just scale it out to the whole dataset and test with random input!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
