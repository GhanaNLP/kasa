{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-v7ZMuVZhYvy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "\n",
    "MODE = 'train'\n",
    "NUM_EPOCHS = 100\n",
    "NUMBER_OF_DATASET = 1000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_WbGFCXhdAE"
   },
   "outputs": [],
   "source": [
    "def read_dataset(number):\n",
    "\n",
    "    english_data = []\n",
    "    with open('/content/drive/My Drive/Dataset/Machine Translation/JW/jw300.en-tw.en') as file:\n",
    "\n",
    "        line = file.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            english_data.append(line.strip())\n",
    "            line = file.readline()\n",
    "            cnt += 1\n",
    "\n",
    "\n",
    "    twi_data = []\n",
    "    with open('/content/drive/My Drive/Dataset/Machine Translation/JW/jw300.en-tw.tw') as file:\n",
    "\n",
    "        # twi=file.read()\n",
    "        line = file.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            twi_data.append(line.strip())\n",
    "            line = file.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return english_data[:number],twi_data[:number]\n",
    "    # return english_data,twi_data÷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yUdlI5ehhxC"
   },
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalize_eng(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n",
    "\n",
    "def normalize_twi(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BZjESAYjhjF2",
    "outputId": "b11377d2-610f-4b85-a2bd-a7f0e813ddd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "raw_data_en,raw_data_twi = read_dataset(NUMBER_OF_DATASET)\n",
    "# raw_data_en, raw_data_twi = list(zip(*raw_data))\n",
    "raw_data_en = [normalize_eng(data) for data in raw_data_en]\n",
    "raw_data_twi_in = ['<start> ' + normalize_twi(data) for data in raw_data_twi]\n",
    "raw_data_twi_out = [normalize_twi(data) + ' <end>' for data in raw_data_twi]\n",
    "\n",
    "print(len(raw_data_twi_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzyEEdhphn2l"
   },
   "outputs": [],
   "source": [
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
    "                                                        padding='post')\n",
    "\n",
    "twi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "twi_tokenizer.fit_on_texts(raw_data_twi_in)\n",
    "twi_tokenizer.fit_on_texts(raw_data_twi_out)\n",
    "data_twi_in = twi_tokenizer.texts_to_sequences(raw_data_twi_in)\n",
    "data_twi_in = tf.keras.preprocessing.sequence.pad_sequences(data_twi_in,\n",
    "                                                           padding='post')\n",
    "\n",
    "data_twi_out = twi_tokenizer.texts_to_sequences(raw_data_twi_out)\n",
    "data_twi_out = tf.keras.preprocessing.sequence.pad_sequences(data_twi_out,\n",
    "                                                            padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qpFaQXR2hpUI",
    "outputId": "b5102123-77bd-44a1-a736-fca4b7b28b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 63), (None, 73), (None, 73)), types: (tf.int32, tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_twi_in, data_twi_out))\n",
    "dataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2J77-BDsh_aS"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Create the Positional Embedding\"\"\"\n",
    "\n",
    "\n",
    "def positional_encoding(pos, model_size):\n",
    "    \"\"\" Compute positional encoding for a particular position\n",
    "\n",
    "    Args:\n",
    "        pos: position of a token in the sequence\n",
    "        model_size: depth size of the model\n",
    "    \n",
    "    Returns:\n",
    "        The positional encoding for the given token\n",
    "    \"\"\"\n",
    "    PE = np.zeros((1, model_size))\n",
    "    for i in range(model_size):\n",
    "        if i % 2 == 0:\n",
    "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
    "        else:\n",
    "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
    "    return PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "BoSZYQDpiEA7",
    "outputId": "2717308b-8e48-4b91-f61c-bde57ce880c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 128)\n",
      "(1000, 63)\n",
      "(1000, 73)\n"
     ]
    }
   ],
   "source": [
    "max_length = max(len(data_en[0]), len(data_twi_in[0]))\n",
    "MODEL_SIZE = 128\n",
    "\n",
    "pes = []\n",
    "for i in range(max_length):\n",
    "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
    "\n",
    "pes = np.concatenate(pes, axis=0)\n",
    "pes = tf.constant(pes, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(pes.shape)\n",
    "print(data_en.shape)\n",
    "print(data_twi_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYwTsioviGzr"
   },
   "outputs": [],
   "source": [
    "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.Model):\n",
    "    \"\"\" Class for Multi-Head Attention layer\n",
    "\n",
    "    Attributes:\n",
    "        key_size: d_key in the paper\n",
    "        h: number of attention heads\n",
    "        wq: the Linear layer for Q\n",
    "        wk: the Linear layer for K\n",
    "        wv: the Linear layer for V\n",
    "        wo: the Linear layer for the output\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, h):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.key_size = model_size // h\n",
    "        self.h = h\n",
    "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
    "        self.wo = tf.keras.layers.Dense(model_size)\n",
    "\n",
    "    def call(self, query, value, mask=None):\n",
    "        \"\"\" The forward pass for Multi-Head Attention layer\n",
    "\n",
    "        Args:\n",
    "            query: the Q matrix\n",
    "            value: the V matrix, acts as V and K\n",
    "            mask: mask to filter out unwanted tokens\n",
    "                  - zero mask: mask for padded tokens\n",
    "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
    "        \n",
    "        Returns:\n",
    "            The concatenated context vector\n",
    "            The alignment (attention) vectors of all heads\n",
    "        \"\"\"\n",
    "        # query has shape (batch, query_len, model_size)\n",
    "        # value has shape (batch, value_len, model_size)\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(value)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        # Split matrices for multi-heads attention\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Originally, query has shape (batch, query_len, model_size)\n",
    "        # We need to reshape to (batch, query_len, h, key_size)\n",
    "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
    "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
    "        query = tf.transpose(query, [0, 2, 1, 3])\n",
    "        \n",
    "        # Do the same for key and value\n",
    "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
    "        key = tf.transpose(key, [0, 2, 1, 3])\n",
    "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
    "        value = tf.transpose(value, [0, 2, 1, 3])\n",
    "        \n",
    "        # Compute the dot score\n",
    "        # and divide the score by square root of key_size (as stated in paper)\n",
    "        # (must convert key_size to float32 otherwise an error would occur)\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
    "        # score will have shape of (batch, h, query_len, value_len)\n",
    "        \n",
    "        # Mask out the score if a mask is provided\n",
    "        # There are two types of mask:\n",
    "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
    "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
    "        if mask is not None:\n",
    "            score *= mask\n",
    "\n",
    "            # We want the masked out values to be zeros when applying softmax\n",
    "            # One way to accomplish that is assign them to a very large negative value\n",
    "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
    "        \n",
    "        # Alignment vector: (batch, h, query_len, value_len)\n",
    "        alignment = tf.nn.softmax(score, axis=-1)\n",
    "        \n",
    "        # Context vector: (batch, h, query_len, key_size)\n",
    "        context = tf.matmul(alignment, value)\n",
    "        \n",
    "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
    "        context = tf.transpose(context, [0, 2, 1, 3])\n",
    "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
    "        \n",
    "        # Apply one last full connected layer (WO)\n",
    "        heads = self.wo(context)\n",
    "        \n",
    "        return heads, alignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "UXWWd0YdiLZS",
    "outputId": "cb8aac4d-dcc9-419d-e93c-913492a4498b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 5, 128])"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"## Create the Encoder\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Encoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention: array of Multi-Head Attention layers\n",
    "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
    "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "\n",
    "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, sequence, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Encoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        sub_in = embed_out\n",
    "        alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
    "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
    "            sub_out = sub_in + sub_out\n",
    "            sub_out = self.attention_norm[i](sub_out)\n",
    "            \n",
    "            alignments.append(alignment)\n",
    "            ffn_in = sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_in + ffn_out\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            sub_in = ffn_out\n",
    "\n",
    "        return ffn_out, alignments\n",
    "\n",
    "\n",
    "H = 8\n",
    "NUM_LAYERS = 4\n",
    "vocab_size = len(en_tokenizer.word_index) + 1\n",
    "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "print(vocab_size)\n",
    "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
    "encoder_output, _ = encoder(sequence_in)\n",
    "encoder_output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrfeLbQEiN0k"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Decoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
    "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
    "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
    "        attention_mid: array of middle Multi-Head Attention layers\n",
    "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
    "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "\n",
    "        dense: Dense layer to compute final output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Decoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            encoder_output: output of the Encoder (for computing middle attention)\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The bottom alignment (attention) vectors for all layers\n",
    "            The middle alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        bot_sub_in = embed_out\n",
    "        bot_alignments = []\n",
    "        mid_alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # BOTTOM MULTIHEAD SUB LAYER\n",
    "            seq_len = bot_sub_in.shape[1]\n",
    "\n",
    "            if training:\n",
    "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            else:\n",
    "                mask = None\n",
    "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
    "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
    "            bot_sub_out = bot_sub_in + bot_sub_out\n",
    "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
    "            \n",
    "            bot_alignments.append(bot_alignment)\n",
    "\n",
    "            # MIDDLE MULTIHEAD SUB LAYER\n",
    "            mid_sub_in = bot_sub_out\n",
    "\n",
    "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
    "                mid_sub_in, encoder_output, encoder_mask)\n",
    "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
    "            mid_sub_out = mid_sub_out + mid_sub_in\n",
    "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
    "            \n",
    "            mid_alignments.append(mid_alignment)\n",
    "\n",
    "            # FFN\n",
    "            ffn_in = mid_sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_out + ffn_in\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            bot_sub_in = ffn_out\n",
    "\n",
    "        logits = self.dense(ffn_out)\n",
    "\n",
    "        return logits, bot_alignments, mid_alignments\n",
    "\n",
    "\n",
    "vocab_size = len(twi_tokenizer.word_index) + 1\n",
    "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "\n",
    "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
    "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
    "decoder_output.shape\n",
    "\n",
    "\n",
    "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEnlzKcOiQgR"
   },
   "outputs": [],
   "source": [
    "\n",
    "def loss_func(targets, logits):\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\" Learning schedule for training the Transformer\n",
    "\n",
    "    Attributes:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        warmup_steps: number of warmup steps at the beginning\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, warmup_steps=4000):\n",
    "        super(WarmupThenDecaySchedule, self).__init__()\n",
    "\n",
    "        self.model_size = model_size\n",
    "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_term = tf.math.rsqrt(step)\n",
    "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_GzxwnniWD7"
   },
   "outputs": [],
   "source": [
    "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(lr,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X3YH3is7iaSA"
   },
   "outputs": [],
   "source": [
    "def predict(test_source_text=None):\n",
    "    \"\"\" Predict the output sentence for a given input sentence\n",
    "\n",
    "    Args:\n",
    "        test_source_text: input sentence (raw string)\n",
    "    \n",
    "    Returns:\n",
    "        The encoder's attention vectors\n",
    "        The decoder's bottom attention vectors\n",
    "        The decoder's middle attention vectors\n",
    "        The input string array (input sentence split by ' ')\n",
    "        The output string array\n",
    "    \"\"\"\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
    "\n",
    "    de_input = tf.constant(\n",
    "        [[twi_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
    "\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
    "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
    "        out_words.append(twi_tokenizer.index_word[new_word.numpy()[0][0]])\n",
    "\n",
    "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
    "        # so we have to add the last predicted word to create a new input sequence\n",
    "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
    "\n",
    "        # TODO: get a nicer constraint for the sequence length!\n",
    "        if out_words[-1] == '<end>':\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))\n",
    "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qoQABG8VidBb"
   },
   "outputs": [],
   "source": [
    "\n",
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out):\n",
    "    \"\"\" Execute one training step (forward pass + backward pass)\n",
    "\n",
    "    Args:\n",
    "        source_seq: source sequences\n",
    "        target_seq_in: input target sequences (<start> + ...)\n",
    "        target_seq_out: output target sequences (... + <end>)\n",
    "    \n",
    "    Returns:\n",
    "        The loss value of the current pass\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
    "        # encoder_mask has shape (batch_size, source_len)\n",
    "        # we need to add two more dimensions in between\n",
    "        # to make it broadcastable when computing attention heads\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
    "\n",
    "        decoder_output, _, _ = decoder(\n",
    "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
    "\n",
    "        loss = loss_func(target_seq_out, decoder_output)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h6hR6rNhigBe",
    "outputId": "8c790a63-12f0-4cb0-be9f-1bdc59fe0b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0703 Elapsed time 13.17s\n",
      "I know I m loved not only by my family but by the congregation and Jehovah as well and that is more special to me than any holiday . Becky age .\n",
      "[[7, 134, 7, 173, 2422, 23, 96, 26, 15, 75, 36, 26, 2, 742, 6, 41, 20, 230, 6, 9, 11, 86, 223, 3, 19, 82, 113, 108, 1, 2423, 78, 1]]\n",
      "nnuaba nnuaba wobegye obiara obiara obiara obiara obiara obiara obiara obiara obiara obiara obiara\n",
      "Epoch 2 Batch 0 Loss 1.9872 Elapsed time 64.24s\n",
      "Notice though what the Bible says at Samuel of David s behavior And David was continually acting prudently in all his ways and Jehovah was with him . \n",
      "[[2999, 201, 62, 2, 68, 121, 22, 3000, 5, 438, 17, 859, 6, 438, 10, 3001, 515, 769, 8, 37, 43, 683, 6, 41, 10, 14, 85, 1]]\n",
      "mmoa wɔfrɛɛ wɔfrɛɛ a a a a a a a a a a a\n",
      "Epoch 3 Batch 0 Loss 2.1002 Elapsed time 59.95s\n",
      "Writer and parent Maureen Orth asks How do we instill values and character in a material world such as ours where consumption and greed seem so glorified often unwittingly ? \n",
      "[[539, 6, 120, 2179, 2180, 2181, 54, 40, 44, 2182, 1075, 6, 606, 8, 4, 1074, 138, 100, 20, 2183, 133, 2184, 6, 1076, 642, 57, 2185, 246, 2186, 34]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 4 Batch 0 Loss 1.9938 Elapsed time 60.34s\n",
      "The kids do a lot of weird things like dress up on Halloween .\n",
      "[[2, 194, 40, 4, 527, 5, 2489, 104, 92, 2490, 56, 16, 196, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 5 Batch 0 Loss 1.9854 Elapsed time 60.04s\n",
      "This is the crucial time for the inculcation of the true moral values .\n",
      "[[33, 11, 2, 2314, 52, 12, 2, 2315, 5, 2, 640, 274, 1075, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 6 Batch 0 Loss 1.8105 Elapsed time 59.95s\n",
      "Few parents are as volatile as King Saul .\n",
      "[[300, 38, 25, 20, 3002, 20, 770, 564, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 7 Batch 0 Loss 1.8262 Elapsed time 59.72s\n",
      "Sometimes though a parent simply isn t disposed to talking things out and all attempts to encourage him or her to do so are met with anger or resistance .\n",
      "[[422, 201, 4, 120, 236, 302, 51, 2988, 3, 1222, 104, 74, 6, 37, 953, 3, 694, 85, 27, 49, 3, 40, 57, 25, 506, 14, 691, 27, 2989, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 8 Batch 0 Loss 1.5192 Elapsed time 60.10s\n",
      "For the last time Mum tucked me into bed and kissed me .\n",
      "[[12, 2, 380, 52, 164, 1457, 19, 87, 333, 6, 1458, 19, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 9 Batch 0 Loss 1.9297 Elapsed time 59.86s\n",
      "How did the celebration of Jesus birth get pushed forward to December to make it coincide blasphemously with the pagan celebration of the birthday of the sun ?\n",
      "[[54, 227, 2, 237, 5, 210, 431, 70, 2527, 1158, 3, 276, 3, 112, 13, 2528, 2529, 14, 2, 143, 237, 5, 2, 1157, 5, 2, 140, 34]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 10 Batch 0 Loss 1.8999 Elapsed time 59.92s\n",
      "Dr . Judith A .\n",
      "[[356, 1, 2142, 4, 1]]\n",
      "<end>\n",
      "Epoch 11 Batch 0 Loss 1.8277 Elapsed time 59.09s\n",
      "Abortion Your series Abortion The Making and the Taking of a Life May brought tears to my eyes and pain to my heart .\n",
      "[[351, 47, 1990, 351, 2, 315, 6, 2, 405, 5, 4, 76, 53, 352, 334, 3, 15, 335, 6, 688, 3, 15, 591, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 12 Batch 0 Loss 1.7939 Elapsed time 59.77s\n",
      "Jehovah s Witnesses cherish their children as gifts from God .\n",
      "[[41, 17, 79, 1086, 35, 45, 20, 109, 32, 64, 1]]\n",
      "a a a . <end>\n",
      "Epoch 13 Batch 0 Loss 1.9276 Elapsed time 58.62s\n",
      "When I became I went into the full time service as a minister of Jehovah s Witnesses and then to Gilead in the United States the Watch Tower Society s school for missionaries .\n",
      "[[28, 7, 366, 7, 214, 87, 2, 262, 52, 263, 20, 4, 505, 5, 41, 17, 79, 6, 77, 3, 1583, 8, 2, 205, 181, 2, 190, 398, 296, 17, 73, 12, 1584, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 14 Batch 0 Loss 1.6670 Elapsed time 58.91s\n",
      "This simple harvesting procedure is familiar to millions throughout the tropics who regularly enjoy cassava .\n",
      "[[33, 672, 1837, 1838, 11, 1839, 3, 1840, 673, 2, 651, 60, 983, 1841, 71, 1]]\n",
      "a a a a a a a a . . . . . .\n",
      "Epoch 15 Batch 0 Loss 1.7740 Elapsed time 58.24s\n",
      "Let s Eat Cassava !\n",
      "[[340, 17, 410, 71, 39]]\n",
      "a a <end>\n",
      "Epoch 16 Batch 0 Loss 1.7478 Elapsed time 58.35s\n",
      "But Ngozi and Janyere decide instead to eat eba which is made by simply stirring the gari into hot water .\n",
      "[[36, 304, 6, 346, 1019, 350, 3, 410, 1958, 180, 11, 129, 26, 236, 1959, 2, 530, 87, 516, 880, 1]]\n",
      "a a a a a a a a a a a a . <end>\n",
      "Epoch 17 Batch 0 Loss 1.6052 Elapsed time 61.29s\n",
      "On December the Mithraists celebrated the birth of Mithra . . .\n",
      "[[16, 276, 2, 2783, 1166, 2, 431, 5, 555, 1, 1, 1]]\n",
      "a a a a a . <end>\n",
      "Epoch 18 Batch 0 Loss 1.5679 Elapsed time 59.76s\n",
      "I don t care .\n",
      "[[7, 193, 51, 416, 1]]\n",
      "a a . <end>\n",
      "Epoch 19 Batch 0 Loss 1.5355 Elapsed time 58.93s\n",
      "My mom and dad get me plenty of toys all the time .\n",
      "[[15, 275, 6, 244, 70, 19, 1129, 5, 1146, 37, 2, 52, 1]]\n",
      "a a a a a a . <end>\n",
      "Epoch 20 Batch 0 Loss 1.4666 Elapsed time 59.27s\n",
      "So I explained to him my Christian beliefs .\n",
      "[[57, 7, 428, 3, 85, 15, 154, 326, 1]]\n",
      "a a a a . <end>\n",
      "Epoch 21 Batch 0 Loss 1.6003 Elapsed time 58.84s\n",
      "Often these foul tasting butterflies like the monarch are brightly colored a visual warning that apparently reminds the bird to keep clear .\n",
      "[[246, 95, 1782, 1783, 139, 92, 2, 973, 25, 974, 666, 4, 1784, 253, 9, 1785, 975, 2, 343, 3, 242, 523, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 22 Batch 0 Loss 1.3881 Elapsed time 59.39s\n",
      "But Ngozi and Janyere decide instead to eat eba which is made by simply stirring the gari into hot water .\n",
      "[[36, 304, 6, 346, 1019, 350, 3, 410, 1958, 180, 11, 129, 26, 236, 1959, 2, 530, 87, 516, 880, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 23 Batch 0 Loss 1.6611 Elapsed time 59.46s\n",
      "The director said I could attend on the condition that I wouldn t tell anyone why I had been expelled from the other school .\n",
      "[[2, 285, 103, 7, 170, 1355, 16, 2, 1356, 9, 7, 1357, 51, 171, 322, 152, 7, 24, 81, 1358, 32, 2, 83, 73, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 24 Batch 0 Loss 1.5422 Elapsed time 60.38s\n",
      "Although the love dust is no guarantee of success it works wonders when a willing female is finally found .\n",
      "[[521, 2, 107, 404, 11, 66, 1705, 5, 954, 13, 955, 1706, 28, 4, 1707, 519, 11, 504, 224, 1]]\n",
      "a a a a a a a a a a a a a a\n",
      "Epoch 25 Batch 0 Loss 1.4806 Elapsed time 60.85s\n",
      "Even so I still love horses .\n",
      "[[97, 57, 7, 248, 107, 418, 1]]\n",
      "sɛ sɛ a a no . <end>\n",
      "Epoch 26 Batch 0 Loss 1.3559 Elapsed time 60.62s\n",
      "Trick or treat The cry of the Druids was comparable to the modern day Trick or Treat . Central Coast Parent .\n",
      "[[1209, 27, 693, 2, 881, 5, 2, 436, 10, 2871, 3, 2, 557, 69, 1209, 27, 693, 1, 2872, 982, 120, 1]]\n",
      "sɛ a a a a a a a a a a na sɛ na\n",
      "Epoch 27 Batch 0 Loss 1.4016 Elapsed time 61.40s\n",
      "Even the meat you eat or the milk you drink may come from animals who have been fed powdered cassava as part of their diet .\n",
      "[[97, 2, 628, 18, 410, 27, 2, 999, 18, 663, 53, 169, 32, 676, 60, 29, 81, 665, 1862, 71, 20, 176, 5, 35, 677, 1]]\n",
      "sɛ na a na na na na na na na na <end>\n",
      "Epoch 28 Batch 0 Loss 1.3089 Elapsed time 60.50s\n",
      "The day arrived when I was called in front of my class to make my decision known .\n",
      "[[2, 69, 814, 28, 7, 10, 219, 8, 364, 5, 15, 168, 3, 112, 15, 589, 363, 1]]\n",
      "sɛ sɛ na na no no no . <end>\n",
      "Epoch 29 Batch 0 Loss 1.2430 Elapsed time 59.35s\n",
      "And many other gifts such as the kitten whose playful antics with a leaf leave us convulsed with laughter or the fuzzy puppy that attacks us head shaking from side to side growling ferociously as it tugs on our sleeve yet with friendly tail wagging nonstop all the while .\n",
      "[[6, 153, 83, 109, 100, 20, 2, 2362, 481, 2363, 2364, 14, 4, 926, 467, 166, 2365, 14, 2366, 27, 2, 2367, 2368, 9, 2369, 166, 590, 2370, 32, 739, 3, 739, 2371, 2372, 20, 13, 2373, 16, 67, 2374, 531, 14, 1128, 2375, 2376, 2377, 37, 2, 165, 1]]\n",
      "na na na na na na na na na na na na na na\n",
      "Epoch 30 Batch 0 Loss 1.2100 Elapsed time 60.20s\n",
      "Early Background\n",
      "[[192, 3113]]\n",
      "na <end>\n",
      "Epoch 31 Batch 0 Loss 1.3851 Elapsed time 58.95s\n",
      "It hurt me to think how Jehovah and Jesus must feel to be connected with such pagan ideas . Alexa age .\n",
      "[[13, 1042, 19, 3, 191, 54, 41, 6, 210, 105, 142, 3, 31, 2471, 14, 100, 143, 2472, 1, 2473, 78, 1]]\n",
      "sɛ na na na na na na na na na na na na na\n",
      "Epoch 32 Batch 0 Loss 1.2948 Elapsed time 59.55s\n",
      "After a family discussion of a Young People Ask . . . article Ruth asked what was bothering her father .\n",
      "[[127, 4, 75, 573, 5, 4, 110, 88, 226, 1, 1, 1, 114, 563, 208, 62, 10, 2968, 49, 198, 1]]\n",
      "sɛ sɛ sɛ na sɛ na sɛ na sɛ na sɛ na sɛ .\n",
      "Epoch 33 Batch 0 Loss 1.3808 Elapsed time 59.29s\n",
      "This does not necessarily mean giving your parents a wide berth .\n",
      "[[33, 141, 23, 1226, 1228, 135, 47, 38, 4, 3014, 3015, 1]]\n",
      "sɛ na na saa a na <end>\n",
      "Epoch 34 Batch 0 Loss 1.3747 Elapsed time 59.24s\n",
      "I especially appreciated the simplicity of presentation .\n",
      "[[7, 1021, 2019, 2, 2020, 5, 2021, 1]]\n",
      "na sɛ na <end>\n",
      "Epoch 35 Batch 0 Loss 1.1803 Elapsed time 59.36s\n",
      "Is this then the way to honor the holy Son of God ?\n",
      "[[11, 33, 77, 2, 89, 3, 435, 2, 2701, 434, 5, 64, 34]]\n",
      "na na na ne no na <end>\n",
      "Epoch 36 Batch 0 Loss 1.2568 Elapsed time 59.25s\n",
      "Vedral A perfect remedy for a grouchy parent is affection . . . .\n",
      "[[1230, 4, 716, 2960, 12, 4, 768, 120, 11, 766, 1, 1, 1, 1]]\n",
      "sɛ na na na na na na sɛ na saa . . . .\n",
      "Epoch 37 Batch 0 Loss 1.3673 Elapsed time 59.26s\n",
      "I get lots of things without having to celebrate holidays .\n",
      "[[7, 70, 1143, 5, 104, 280, 243, 3, 238, 122, 1]]\n",
      "sɛ na saa a me no . <end>\n",
      "Epoch 38 Batch 0 Loss 1.1343 Elapsed time 59.17s\n",
      "Two ladies took me into custody .\n",
      "[[150, 858, 258, 19, 87, 1434, 1]]\n",
      "na me me . <end>\n",
      "Epoch 39 Batch 0 Loss 1.3195 Elapsed time 59.08s\n",
      " When I missed holidays at school I wasn t regretful .\n",
      "[[28, 7, 2487, 122, 22, 73, 7, 324, 51, 2488, 1]]\n",
      "sɛ na me no na me no . <end>\n",
      "Epoch 40 Batch 0 Loss 1.1679 Elapsed time 59.20s\n",
      "It is going to be your schooling for future life .\n",
      "[[13, 11, 213, 3, 31, 47, 1443, 12, 621, 76, 1]]\n",
      "sɛ na ɛyɛ no na ɛyɛ no . <end>\n",
      "Epoch 41 Batch 0 Loss 1.1715 Elapsed time 59.46s\n",
      "This is not laziness .\n",
      "[[33, 11, 23, 1628, 1]]\n",
      "sɛ na saa a na saa no . <end>\n",
      "Epoch 42 Batch 0 Loss 1.0460 Elapsed time 59.32s\n",
      "When I was to be sentenced to prison for the third time petitions were filed for me to be tried as a conscientious objector .\n",
      "[[28, 7, 10, 3, 31, 479, 3, 772, 12, 2, 1251, 52, 3101, 55, 3102, 12, 19, 3, 31, 256, 20, 4, 3103, 3104, 1]]\n",
      "na sɛ na me ne wɔn a na na me no na saa .\n",
      "Epoch 43 Batch 0 Loss 1.0713 Elapsed time 59.26s\n",
      "In fact the work involved from harvest to dinner table may equal or surpass all preharvest activities .\n",
      "[[8, 681, 2, 90, 1011, 32, 680, 3, 1912, 602, 53, 1913, 27, 1914, 37, 1915, 1916, 1]]\n",
      "na sɛ na saa a na saa a na saa so no mu no\n",
      "Epoch 44 Batch 0 Loss 1.0524 Elapsed time 58.73s\n",
      "She was honored by drunken sex orgies the worshipers believing that their sexual intercourse helped to bring about the full awakening and mating of Baal with his wife .\n",
      "[[48, 10, 1176, 26, 2670, 307, 2671, 2, 553, 2672, 9, 35, 2673, 2674, 486, 3, 427, 42, 2, 262, 2675, 6, 2676, 5, 1187, 14, 43, 411, 1]]\n",
      "na sɛ wɔde no na ɛyɛ no na ɛyɛ sɛ ɛyɛ den sɛ ɛyɛ\n",
      "Epoch 45 Batch 0 Loss 1.0198 Elapsed time 59.14s\n",
      "In this way he will grow to believe the world owes him a living says the pamphlet Rules for Raising Delinquent Children .\n",
      "[[8, 33, 89, 21, 61, 349, 3, 355, 2, 138, 2176, 85, 4, 252, 121, 2, 2177, 588, 12, 1073, 595, 45, 1]]\n",
      "sɛ na saa a na saa no na saa a na saa saa saa\n",
      "Epoch 46 Batch 0 Loss 1.0073 Elapsed time 59.35s\n",
      "Later I was appointed to sort paper cans and bones .\n",
      "[[157, 7, 10, 849, 3, 1412, 615, 850, 6, 851, 1]]\n",
      "me ne sɛ wɔde me kɔɔ hɔ . <end>\n",
      "Epoch 47 Batch 0 Loss 0.9436 Elapsed time 59.13s\n",
      " You can t believe everything the TV ad says . \n",
      "[[18, 58, 51, 355, 546, 2, 704, 2154, 121, 1]]\n",
      "sɛ na saa a ɛyɛ no so no so . <end>\n",
      "Epoch 48 Batch 0 Loss 1.0696 Elapsed time 59.35s\n",
      "All at once the tears I had kept inside of me for the past months burst out .\n",
      "[[37, 22, 188, 2, 334, 7, 24, 485, 1552, 5, 19, 12, 2, 1553, 233, 1554, 74, 1]]\n",
      "na me no na me ma me sɛ na ɔde . <end>\n",
      "Epoch 49 Batch 0 Loss 0.8835 Elapsed time 59.56s\n",
      "I began to cry again .\n",
      "[[7, 247, 3, 881, 250, 1]]\n",
      "na sɛ edi hɔ . <end>\n",
      "Epoch 50 Batch 0 Loss 0.9866 Elapsed time 59.46s\n",
      "But I wasn t !\n",
      "[[36, 7, 324, 51, 39]]\n",
      "nanso nanso ! <end>\n",
      "Epoch 51 Batch 0 Loss 0.8855 Elapsed time 59.44s\n",
      "In addition to its contribution to the food industry cassava is used in making adhesives pastes and paints .\n",
      "[[8, 1863, 3, 174, 1864, 3, 2, 390, 1865, 71, 11, 331, 8, 315, 1866, 1867, 6, 1868, 1]]\n",
      "sɛ wɔde no wɔn a wɔde me ne wɔn a wɔde wɔn ho so\n",
      "Epoch 52 Batch 0 Loss 0.8604 Elapsed time 59.97s\n",
      "The book explained that man is a soul and does not possess a separate invisible soul .\n",
      "[[2, 132, 428, 9, 131, 11, 4, 565, 6, 141, 23, 3132, 4, 3133, 3134, 565, 1]]\n",
      "sɛ na saa no saa bere a na ɛyɛ den sɛ ɔyɛ no so\n",
      "Epoch 53 Batch 0 Loss 1.0499 Elapsed time 60.51s\n",
      "All at once the tears I had kept inside of me for the past months burst out .\n",
      "[[37, 22, 188, 2, 334, 7, 24, 485, 1552, 5, 19, 12, 2, 1553, 233, 1554, 74, 1]]\n",
      "na sɛ me kɔɔ hɔ a na me no na enti na enti na\n",
      "Epoch 54 Batch 0 Loss 0.8166 Elapsed time 61.02s\n",
      "One encouraged parents to tell their children the truth by age six or seven warning that it may actually be harmful to their psyches if parents persist in perpetuating the myth . \n",
      "[[50, 1053, 38, 3, 171, 35, 45, 2, 179, 26, 78, 362, 27, 533, 253, 9, 13, 53, 2124, 31, 710, 3, 35, 2125, 99, 38, 2126, 8, 2127, 2, 706, 1]]\n",
      "bere a na wɔn no mu sɛ na wɔn mma sɛ awofo no so\n",
      "Epoch 55 Batch 0 Loss 0.8624 Elapsed time 60.56s\n",
      "He would also ask for the names of the other Witnesses .\n",
      "[[21, 72, 172, 226, 12, 2, 603, 5, 2, 83, 79, 1]]\n",
      "afei na sɛ anka wɔteɛɛm hɔ no mu . <end>\n",
      "Epoch 56 Batch 0 Loss 0.8251 Elapsed time 60.78s\n",
      "It caused quite a disturbance at school .\n",
      "[[13, 577, 365, 4, 1302, 22, 73, 1]]\n",
      "saa me ma sukuu hɔ . <end>\n",
      "Epoch 57 Batch 0 Loss 0.8164 Elapsed time 59.89s\n",
      " Happy is the people whose God is Jehovah ! Psalm .\n",
      "[[305, 11, 2, 88, 481, 64, 11, 41, 39, 787, 1]]\n",
      "ɛyɛ yehowa ne yehowa adansefo no ! <end>\n",
      "Epoch 58 Batch 0 Loss 0.9191 Elapsed time 59.61s\n",
      "Holly was popular with the Celts to keep the house goblins in order at winter solstice time . . . .\n",
      "[[2766, 10, 1180, 14, 2, 1194, 3, 242, 2, 147, 2767, 8, 736, 22, 336, 1162, 52, 1, 1, 1, 1]]\n",
      "na sɛ saa wɔ mu no mu no na ɔde ne bere a na\n",
      "Epoch 59 Batch 0 Loss 0.8121 Elapsed time 59.52s\n",
      "There is absolutely no biblical authority for December as having been the day of the Nativity . Isaac Asimov .\n",
      "[[63, 11, 2784, 66, 1098, 2785, 12, 276, 20, 243, 81, 2, 69, 5, 2, 1195, 1, 2786, 2787, 1]]\n",
      "nanso da bi yɛ den ma wɔn ho so na wɔn ho asɛm sɛ\n",
      "Epoch 60 Batch 0 Loss 0.7831 Elapsed time 59.81s\n",
      "They may often be found sucking up the salty moisture from a patch of wet ground or occasionally the perspiration on the hand of a human admirer .\n",
      "[[30, 53, 246, 31, 224, 1735, 56, 2, 1736, 960, 32, 4, 1737, 5, 877, 234, 27, 961, 2, 1738, 16, 2, 299, 5, 4, 406, 1739, 1]]\n",
      "sɛ afofantɔ a afofantɔ ne nneɛma a ɛyɛ den sɛ ɛne wɔn mma fɛ\n",
      "Epoch 61 Batch 0 Loss 0.6750 Elapsed time 59.73s\n",
      "Perhaps more diligence on your part would do much to improve your parents mood .\n",
      "[[420, 86, 2987, 16, 47, 176, 72, 40, 155, 3, 763, 47, 38, 212, 1]]\n",
      "sɛ saa saa yi ma adwuma no so nya a ɛyɛ den saa so\n",
      "Epoch 62 Batch 0 Loss 0.6423 Elapsed time 59.97s\n",
      "From Dachau he was sent to Mauthausen an extermination camp worse than Dachau where he suffered hard labor and beatings and was set upon by police dogs .\n",
      "[[32, 638, 21, 10, 320, 3, 1577, 59, 1578, 327, 913, 82, 638, 133, 21, 1579, 231, 639, 6, 1580, 6, 10, 338, 487, 26, 187, 1581, 1]]\n",
      "na bere a edi hɔ no me no na ɔde me wɔ hɔ .\n",
      "Epoch 63 Batch 0 Loss 0.7306 Elapsed time 60.16s\n",
      "For want of a better phrase I d call it contentment . \n",
      "[[12, 257, 5, 4, 303, 2278, 7, 532, 379, 13, 2279, 1]]\n",
      "sɛ anka nhwɛso no ebia aban no sɛ so sɛ so . <end>\n",
      "Epoch 64 Batch 0 Loss 0.7999 Elapsed time 63.02s\n",
      "Cultures the world over have festivals for the dead held by all on or about the very day on which according to the Mosaic account the Deluge took place viz . the seventeenth day of the second month the month nearly corresponding with our November . The Worship of the Dead by J .\n",
      "[[2732, 2, 138, 117, 29, 432, 12, 2, 261, 574, 26, 37, 16, 27, 42, 2, 200, 69, 16, 180, 222, 3, 2, 2733, 805, 2, 2734, 258, 600, 2735, 1, 2, 2736, 69, 5, 2, 575, 283, 2, 283, 2737, 2738, 14, 67, 872, 1, 2, 197, 5, 2, 261, 26, 537, 1]]\n",
      "sɛnea mmofra no na wogye abosonsomfo afahyɛ ahorow a na wɔn ho asɛm a\n",
      "Epoch 65 Batch 0 Loss 0.6399 Elapsed time 62.45s\n",
      "The two doctors sat behind a table I sat with a bright light shining in my face and the cross examination began .\n",
      "[[2, 150, 1379, 831, 460, 4, 602, 7, 831, 14, 4, 475, 381, 1380, 8, 15, 382, 6, 2, 321, 802, 247, 1]]\n",
      "na druidfo de akanea bi yɛ den yɛ den sɛ na druidfo de wɔn\n",
      "Epoch 66 Batch 0 Loss 0.5554 Elapsed time 62.53s\n",
      "For instance young Sam says My father s not a Christian and he has some temper !\n",
      "[[12, 1120, 110, 3004, 121, 15, 198, 17, 23, 4, 154, 6, 21, 93, 65, 3005, 39]]\n",
      "sɛ saa bere a me kɔɔ hɔ yɛ ! <end>\n",
      "Epoch 67 Batch 0 Loss 0.6819 Elapsed time 61.78s\n",
      "Eating is a compulsion for me a drug .\n",
      "[[268, 11, 4, 2030, 12, 19, 4, 1034, 1]]\n",
      "akyiri yi me ma papa . <end>\n",
      "Epoch 68 Batch 0 Loss 0.6362 Elapsed time 61.39s\n",
      "Was he worthy ?\n",
      "[[10, 21, 2598, 34]]\n",
      "so na so ? <end>\n",
      "Epoch 69 Batch 0 Loss 0.5662 Elapsed time 61.22s\n",
      "But I m not jealous of them .\n",
      "[[36, 7, 173, 23, 1151, 5, 46, 1]]\n",
      "nanso nanso da wɔn mma sɛ afofantɔ . <end>\n",
      "Epoch 70 Batch 0 Loss 0.5472 Elapsed time 61.55s\n",
      "As it turns out the little boy next door was the culprit who revealed the awful truth and put these parents in this awkward position .\n",
      "[[20, 13, 1044, 74, 2, 137, 544, 183, 249, 10, 2, 2076, 60, 2077, 2, 2078, 179, 6, 228, 95, 38, 8, 33, 2079, 2080, 1]]\n",
      "nea edi hɔ no na ade bi a wɔyɛ nea ɛwɔ ne nea ne\n",
      "Epoch 71 Batch 0 Loss 0.6281 Elapsed time 61.53s\n",
      "Further guidance is given at Proverbs which reads Anyone holding back his sayings is possessed of knowledge and a man of discernment is cool of spirit . \n",
      "[[298, 3049, 11, 288, 22, 211, 180, 3050, 322, 3051, 221, 43, 3052, 11, 3053, 5, 3054, 6, 4, 131, 5, 1217, 11, 932, 5, 3055, 1]]\n",
      "nea edi ne nea ɛwɔ ne nea ɛwɔ ne buronya no mu na nea\n",
      "Epoch 72 Batch 0 Loss 0.6195 Elapsed time 61.84s\n",
      "The Bible puts it this way Shrewd is the one that has seen the calamity and proceeds to conceal himself . Proverbs .\n",
      "[[2, 68, 671, 13, 33, 89, 3009, 11, 2, 50, 9, 93, 3010, 2, 3011, 6, 3012, 3, 3013, 318, 1, 211, 1]]\n",
      "bible no ka no saa kwan yi so sɛ na saa bere a saa\n",
      "Epoch 73 Batch 0 Loss 0.5284 Elapsed time 61.33s\n",
      "No wonder I was called the slowest child they ever had !\n",
      "[[66, 496, 7, 10, 219, 2, 1497, 80, 30, 393, 24, 39]]\n",
      "na eyi a ɛte saa akyi no na ɛsɛ sɛ mede me ho ma\n",
      "Epoch 74 Batch 0 Loss 0.4993 Elapsed time 61.60s\n",
      "Even if a fire should burn it to the ground cassava sprouts afresh from its base !\n",
      "[[97, 99, 4, 1007, 392, 1008, 13, 3, 2, 234, 71, 1909, 1910, 32, 174, 957, 39]]\n",
      "sɛ anka bankye ma wo ma aba bankye ! <end>\n",
      "Epoch 75 Batch 0 Loss 0.4931 Elapsed time 61.37s\n",
      "Lear s magazine calls the holiday the most famous wine orgy in the ancient world . \n",
      "[[2606, 17, 414, 753, 2, 108, 2, 162, 2607, 2608, 2609, 8, 2, 240, 138, 1]]\n",
      "nea edi no no na ade a ɛyɛ ade titiriw a ɛyɛ den no\n",
      "Epoch 76 Batch 0 Loss 0.4510 Elapsed time 61.58s\n",
      "Proverbs says By wisdom a household will be built up and by discernment it will prove firmly established . \n",
      "[[211, 121, 26, 2912, 4, 765, 61, 31, 2913, 56, 6, 26, 1217, 13, 61, 2914, 2915, 2916, 1]]\n",
      "nea wɔde no na nea wɔde me kɔɔ so a wɔde ka kyerɛ me\n",
      "Epoch 77 Batch 0 Loss 0.4102 Elapsed time 61.27s\n",
      "Halloween parties and the telling of scary tales also have their origin in the Druid times when spirits were believed to be abroad in the land . The Tampa Tribune .\n",
      "[[196, 358, 6, 2, 1060, 5, 1210, 2873, 172, 29, 35, 2874, 8, 2, 761, 151, 28, 430, 55, 2875, 3, 31, 2876, 8, 2, 631, 1, 2, 1207, 1208, 1]]\n",
      "sɛ afofantɔ a wɔde yɛ den a ɛwɔ owia ne nnipa afahyɛ ahorow yɛ\n",
      "Epoch 78 Batch 0 Loss 0.4224 Elapsed time 61.06s\n",
      "From Our Readers\n",
      "[[32, 67, 1989]]\n",
      "efi yɛn <end>\n",
      "Epoch 79 Batch 0 Loss 0.3836 Elapsed time 60.25s\n",
      "She said I give your girl back to you in the same mental attitude she came . \n",
      "[[48, 103, 7, 160, 47, 124, 221, 3, 18, 8, 2, 218, 1558, 909, 48, 98, 1]]\n",
      "afei awuraa messinger de no me ma me nsa yɛ den sɛ menkyerɛkyerɛ nea\n",
      "Epoch 80 Batch 0 Loss 0.3642 Elapsed time 61.20s\n",
      "Never talk back or answer insolently .\n",
      "[[184, 291, 221, 27, 373, 1441, 1]]\n",
      "nkasa wɔ animtiaabu so anaa nyi nsɛm ano asa so . <end>\n",
      "Epoch 81 Batch 0 Loss 0.3308 Elapsed time 61.25s\n",
      "\n",
      "[[]]\n",
      "afei na sɛ mmofra no mu yɛ den koraa no nam saa awofo bɛyɛ\n",
      "Epoch 82 Batch 0 Loss 0.4092 Elapsed time 61.74s\n",
      "At times you may feel that you must walk on eggs around them nervously awaiting the next time you will get criticized yelled at or even blamed . The article Why Are My Parents So Moody ? in the preceding issue of Awake !\n",
      "[[22, 151, 18, 53, 142, 9, 18, 105, 871, 16, 310, 332, 46, 2900, 2901, 2, 183, 52, 18, 61, 70, 1096, 2902, 22, 27, 97, 2903, 1, 2, 114, 152, 25, 15, 38, 57, 559, 34, 8, 2, 2904, 698, 5, 669, 39]]\n",
      "sɛ edi kan a ebia ɔsɛe a edi hɔ no ebia ɛbɛyɛ na wɔwɔ\n",
      "Epoch 83 Batch 0 Loss 0.3106 Elapsed time 61.58s\n",
      "A World Herald article explains why John it seemed had learned earlier that day that Santa Claus wasn t real .\n",
      "[[4, 138, 421, 114, 708, 152, 707, 13, 375, 24, 478, 709, 9, 69, 9, 163, 354, 324, 51, 220, 1]]\n",
      "sɛ saa nhoma no nhoma kɔkɔbɔ sukuu dan mu no nyinaa sɛ na saa\n",
      "Epoch 84 Batch 0 Loss 0.3259 Elapsed time 62.86s\n",
      " When I missed holidays at school I wasn t regretful .\n",
      "[[28, 7, 2487, 122, 22, 73, 7, 324, 51, 2488, 1]]\n",
      "sɛ nhwɛso no eyi yɛ sukuu dan mu . <end>\n",
      "Epoch 85 Batch 0 Loss 0.3247 Elapsed time 62.93s\n",
      "A Sunshine Breakfast\n",
      "[[4, 510, 884]]\n",
      "ɔto awia anɔpa <end>\n",
      "Epoch 86 Batch 0 Loss 0.3028 Elapsed time 62.76s\n",
      "The children were forbidden to talk to one another and were not allowed to be alone not even to go to the toilet .\n",
      "[[2, 45, 55, 835, 3, 291, 3, 50, 158, 6, 55, 23, 594, 3, 31, 447, 23, 97, 3, 101, 3, 2, 888, 1]]\n",
      "na bere biara nni ne maame ka kyerɛ wɔn na na na easter ne\n",
      "Epoch 87 Batch 0 Loss 0.2585 Elapsed time 63.40s\n",
      "No Mixing of Truth With Falsehood\n",
      "[[66, 2570, 5, 179, 14, 1130]]\n",
      "ɛnsɛ sɛ wɔde atoro frafra nokware <end>\n",
      "Epoch 88 Batch 0 Loss 0.2474 Elapsed time 61.84s\n",
      " January and while I m very happy that the writer is managing his battle with his weight I fear the article will encourage many to think that overweight people simply need self control and to diet .\n",
      "[[2023, 6, 165, 7, 173, 200, 305, 9, 2, 539, 11, 2024, 43, 2025, 14, 43, 538, 7, 1031, 2, 114, 61, 694, 153, 3, 191, 9, 2026, 88, 236, 266, 1032, 2027, 6, 3, 677, 1]]\n",
      "bere a na yehowa adansefo no wɔn mu gyidi kyerɛ wɔn bere mu sɛnea\n",
      "Epoch 89 Batch 0 Loss 0.2271 Elapsed time 62.14s\n",
      "Children dressing up as devils comic book characters what for ?\n",
      "[[45, 2505, 56, 20, 2506, 2507, 132, 2508, 62, 12, 34]]\n",
      "mmofra a wɔyɛ ho asɛm ? <end>\n",
      "Epoch 90 Batch 0 Loss 0.2363 Elapsed time 61.53s\n",
      "Youthful misbehavior and a deteriorating attitude toward adults go hand in hand .\n",
      "[[550, 783, 6, 4, 2239, 909, 514, 711, 101, 299, 8, 299, 1]]\n",
      "mmofrabɔnesɛm na ɛmma wommu mpanyimfo . <end>\n",
      "Epoch 91 Batch 0 Loss 0.1638 Elapsed time 61.81s\n",
      "However showing his bias he said I want it to go on record that I believe it is dangerous to have such a fanatic as you loose in the community . \n",
      "[[186, 1229, 43, 3107, 21, 103, 7, 257, 13, 3, 101, 16, 3108, 9, 7, 355, 13, 11, 3109, 3, 29, 100, 4, 3110, 20, 18, 3111, 8, 2, 3112, 1]]\n",
      "nanso bere a ɔyɛ ɔkwampaefo anaa me mfɛfo sukuufo no ho asɛm sɛ wɔde\n",
      "Epoch 92 Batch 0 Loss 0.1609 Elapsed time 62.33s\n",
      "Wait until he is and then let him decide for himself . \n",
      "[[512, 259, 21, 11, 6, 77, 340, 85, 1019, 12, 318, 1]]\n",
      "twɛn kosi sɛ obedi mfe na afei ma ɔno ankasa nsi ne gyinae .\n",
      "Epoch 93 Batch 0 Loss 0.1567 Elapsed time 62.00s\n",
      "Parents do you want your children imitating these sinister rituals ?\n",
      "[[38, 40, 18, 257, 47, 45, 2757, 95, 2758, 1193, 34]]\n",
      "awofo so mo hu ? <end>\n",
      "Epoch 94 Batch 0 Loss 0.1767 Elapsed time 61.29s\n",
      "Let s Eat Cassava !\n",
      "[[340, 17, 410, 71, 39]]\n",
      "momma yenni bankye ! <end>\n",
      "Epoch 95 Batch 0 Loss 0.1527 Elapsed time 61.19s\n",
      "\n",
      "[[]]\n",
      "afei kae ba sɛ buronya ho dwuma ɔkwammɔne so na wɔma etumi di sɛ\n",
      "Epoch 96 Batch 0 Loss 0.1275 Elapsed time 60.93s\n",
      "As soon as the German army entered France over the Belgium border we began to see swastikas on flags on top of churches even though the French flag still flew over city hall .\n",
      "[[20, 792, 20, 2, 457, 793, 1307, 570, 117, 2, 1308, 1309, 44, 247, 3, 149, 1310, 16, 1311, 16, 794, 5, 795, 97, 201, 2, 458, 1312, 248, 1313, 117, 796, 459, 1]]\n",
      "bere a na ehu aka me no mekɔɔ adwuma no huu sɛ bere a\n",
      "Epoch 97 Batch 0 Loss 0.1208 Elapsed time 61.26s\n",
      "IN February I was in prison in Adelaide South Australia for refusing to bear arms during World War II .\n",
      "[[8, 1039, 7, 10, 8, 772, 8, 241, 566, 439, 12, 848, 3, 840, 369, 144, 138, 394, 1246, 1]]\n",
      "wɔ february mu no esiane parents no sɛ mɛfa wiase esiane sɛ mepow nya\n",
      "Epoch 98 Batch 0 Loss 0.1203 Elapsed time 60.59s\n",
      "First she peels the cassava with a knife then she washes it .\n",
      "[[159, 48, 1923, 2, 71, 14, 4, 1924, 77, 48, 1925, 13, 1]]\n",
      "odi kan de sekamma sinsen bankye no afei ɔhohoro ho . <end>\n",
      "Epoch 99 Batch 0 Loss 0.1100 Elapsed time 60.65s\n",
      "Surprisingly enough the female isn t that fussy about the fancy colors of her male consort .\n",
      "[[1676, 1677, 2, 519, 302, 51, 9, 1678, 42, 2, 1679, 654, 5, 49, 402, 1680, 1]]\n",
      "nea ɛyɛ nwonwa yiye ne sɛ ɔbere no ani nnye ne hokafo nini no\n",
      "Epoch 100 Batch 0 Loss 0.1088 Elapsed time 60.36s\n",
      "A pediatrician describes a different family They want kids who care about others and who give a little of themselves . . . .\n",
      "[[4, 2275, 2276, 4, 401, 75, 30, 257, 194, 60, 416, 42, 185, 6, 60, 160, 4, 137, 5, 472, 1, 1, 1, 1]]\n",
      "oduruyɛfo bi ka abusua foforo bi ho asɛm sɛ wɔpɛ mmofra a wodwen afoforo\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "starttime = time.time()\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out)\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
    "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
    "            starttime = time.time()\n",
    "\n",
    "    try:\n",
    "        predict()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N_Zt6Nn_ikl6",
    "outputId": "f9f4649c-a2d4-42f6-84aa-939099944e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Oh Jehovah Keep My Young Girl Faithful ! \n",
      "[[441, 41, 242, 15, 110, 124, 442, 39]]\n",
      "oo yehowa boa me babea kumaa yi ma onni nokware ! <end>\n",
      "\n",
      "I WAS born in in Alsace France into an artistic family .\n",
      "[[7, 10, 443, 8, 8, 1270, 570, 87, 59, 1271, 75, 1]]\n",
      "wɔwoo me too abusua a wonim adwinne di mu wɔ alsace france wɔ mu . <end>\n",
      "\n",
      "During the evenings Father sitting in his lounge chair would be reading some books about geography or astronomy .\n",
      "[[144, 2, 776, 198, 1272, 8, 43, 1273, 1274, 72, 31, 312, 65, 777, 42, 1275, 27, 1276, 1]]\n",
      "na papa taa anaa kenkan asase ho nsɛm ho asɛm bi ho so asram taa fuw afahyɛ foforo ma abusuabɔ pii . <end>\n",
      "\n",
      "My doggy would be sleeping by his feet and Daddy would be sharing with Mum some highlights from his reading while she was knitting for her family .\n",
      "[[15, 1277, 72, 31, 1278, 26, 43, 313, 6, 360, 72, 31, 571, 14, 164, 65, 1279, 32, 43, 312, 165, 48, 10, 1280, 12, 49, 75, 1]]\n",
      "na me kraman da papa nan so na na ɔka nsɛntitiriw a epue wɔ n’akenkan no mu kyerɛ maame bere a ɔnwene abusua no nneɛma no . <end>\n",
      "\n",
      "How much I enjoyed those evenings !\n",
      "[[54, 155, 7, 778, 115, 776, 39]]\n",
      "na m’ani gye anwummere a ɛtete saa no ho kɛse ! <end>\n",
      "\n",
      "Religion played a big part in our lives .\n",
      "[[572, 1281, 4, 278, 176, 8, 67, 444, 1]]\n",
      "na yɛpɛ nyamesom kɛse . <end>\n",
      "\n",
      "We were staunch Catholics and people who saw us going to church on Sunday morning would say It s nine o clock .\n",
      "[[44, 55, 779, 1282, 6, 88, 60, 361, 166, 213, 3, 167, 16, 445, 145, 72, 125, 13, 17, 780, 781, 446, 1]]\n",
      "na na mmofra no nso wɔ ɔsram biara nkutoo na kɔ yiye na nso no ase kɔ yiye . <end>\n",
      "\n",
      "The Arnolds are going to church . \n",
      "[[2, 1283, 25, 213, 3, 167, 1]]\n",
      "arnold asɔre bio na abusua no nyinaa . <end>\n",
      "\n",
      "Every day before going to school I went to church .\n",
      "[[177, 69, 111, 213, 3, 73, 7, 214, 3, 167, 1]]\n",
      "na mekɔ asɔre anɔpa biara ansa na makɔ sukuu . <end>\n",
      "\n",
      "But because of the priest s misbehavior Mum forbade me to go to church alone .\n",
      "[[36, 146, 5, 2, 782, 17, 783, 164, 1284, 19, 3, 101, 3, 167, 447, 1]]\n",
      "nanso esiane ɔsɔfo no suban bɔne nti maame amma me nkutoo ankɔ asɔre bio . <end>\n",
      "\n",
      "I was six years old at the time .\n",
      "[[7, 10, 362, 126, 178, 22, 2, 52, 1]]\n",
      "saa bere no na madi mfe asia . <end>\n",
      "\n",
      "After having read only three booklets of the Bibelforscher Bible Students now known as Jehovah s Witnesses my mother started preaching from house to house .\n",
      "[[127, 243, 116, 96, 279, 1285, 5, 2, 1286, 68, 448, 128, 363, 20, 41, 17, 79, 15, 102, 314, 449, 32, 147, 3, 147, 1]]\n",
      "bere a na misua no na abofra ba sukuu dan mu adekyɛ ho asɛm bi ho asɛm kyerɛ me maame so . <end>\n",
      "\n",
      "Dad was upset about that .\n",
      "[[244, 10, 450, 42, 9, 1]]\n",
      "eyi haw me papa . <end>\n",
      "\n",
      "He made it a rule that no religious discussion was to be held in front of me .\n",
      "[[21, 129, 13, 4, 784, 9, 66, 199, 573, 10, 3, 31, 574, 8, 364, 5, 19, 1]]\n",
      "ɔhyɛɛ mmara sɛ wɔmmmɔ ɔsom ho nkɔmmɔ biara bere a mewɔ hɔ . <end>\n",
      "\n",
      " No reading of that stuff ! \n",
      "[[66, 312, 5, 9, 1287, 39]]\n",
      "monnkenkan saa nhoma a mfaso biara nni so no . <end>\n",
      "\n",
      "But Mother was so enthusiastic about the truth that she decided to do some Bible reading with me .\n",
      "[[36, 102, 10, 57, 1288, 42, 2, 179, 9, 48, 1289, 3, 40, 65, 68, 312, 14, 19, 1]]\n",
      "nanso na maame ani gye nokware no ho araa ma ɔne me boom kenkan bible no . <end>\n",
      "\n",
      "She got a Catholic version of the Bible and read it every morning without commenting on it to obey Dad .\n",
      "[[48, 130, 4, 451, 785, 5, 2, 68, 6, 116, 13, 177, 145, 280, 1290, 16, 13, 3, 786, 244, 1]]\n",
      "na ɔwɔ katolekfo bible nkyerɛase no bi na na na ɔkenkan no anɔpa biara a na ɔkenkan no ma wɔbɔɔ me kɔɔ ase yɛɛ den wɔ adwumayɛ mu na meba . <end>\n",
      "\n",
      "One day she read Psalm Their idols are silver and gold the work of the hands of earthling man . . . .\n",
      "[[50, 69, 48, 116, 787, 35, 1291, 25, 1292, 6, 1293, 2, 90, 5, 2, 452, 5, 1294, 131, 1, 1, 1, 1]]\n",
      "da bi ɔkenkan dwom wɔn ahoni yɛ dwetɛ ne sika nnipa nsa ano adwuma . . . <end>\n",
      "\n",
      "Those making them will become just like them all those who are trusting in them . \n",
      "[[115, 315, 46, 61, 245, 156, 92, 46, 37, 115, 60, 25, 1295, 8, 46, 1]]\n",
      "wɔn a wɔyɛ wɔn te sɛ wɔn ara obiara a ɔde ne ho to wɔn so no . <end>\n",
      "\n",
      "She linked it with the second commandment which states You must not make for yourself a carved image . \n",
      "[[48, 1296, 13, 14, 2, 575, 1297, 180, 181, 18, 105, 23, 112, 12, 453, 4, 1298, 1299, 1]]\n",
      "ɔde toaa mmara asɛm a etia abien no so nea ɛka sɛ nyɛ anigye no ano no . <end>\n",
      "\n",
      "I immediately got up and destroyed my personal altar I had in my room .\n",
      "[[7, 576, 130, 56, 6, 1300, 15, 1301, 788, 7, 24, 8, 15, 316, 1]]\n",
      "mesɔree ntɛm ara kɔsɛee m’ankasa m’afɔrepon a ɛwɔ me dan mu no . <end>\n",
      "\n",
      "I would go to school and share with my Catholic classmates my daily Bible reading .\n",
      "[[7, 72, 101, 3, 73, 6, 789, 14, 15, 451, 281, 15, 454, 68, 312, 1]]\n",
      "mekɔ sukuu a na me ne me mfɛfo sukuufo a wɔyɛ katolekfo no bom susuw me da biara da bible akenkan no ho . <end>\n",
      "\n",
      "It caused quite a disturbance at school .\n",
      "[[13, 577, 365, 4, 1302, 22, 73, 1]]\n",
      "eyi de ɔhaw kɛse baa sukuu hɔ . <end>\n",
      "\n",
      "Very often children would follow me in the street calling me a stinky Jew ! \n",
      "[[200, 246, 45, 72, 1303, 19, 8, 2, 1304, 578, 19, 4, 1305, 790, 39]]\n",
      "na mmofra taa di m’akyi wɔ abɔnten so frɛ me yudani bɔmɔne ! <end>\n",
      "\n",
      "That was in .\n",
      "[[9, 10, 8, 1]]\n",
      "na eyi yɛ . <end>\n",
      "\n",
      "This situation caused my father to check on what I was learning .\n",
      "[[33, 317, 577, 15, 198, 3, 1306, 16, 62, 7, 10, 791, 1]]\n",
      "saa tebea yi maa papa yɛɛ nhwehwɛmu wɔ nea na meresua no ho . <end>\n",
      "\n",
      "He got himself the book Creation published by Jehovah s Witnesses .\n",
      "[[21, 130, 318, 2, 132, 455, 456, 26, 41, 17, 79, 1]]\n",
      "ɔno ankasa a yehowa adansefo ba no nyinaa ! <end>\n",
      "\n",
      "He read it and became a Witness himself !\n",
      "[[21, 116, 13, 6, 366, 4, 148, 318, 39]]\n",
      "ɔkenkanee na ɔno ankasa bɛyɛɛ ɔdansefo ! <end>\n",
      "\n",
      "As soon as the German army entered France over the Belgium border we began to see swastikas on flags on top of churches even though the French flag still flew over city hall .\n",
      "[[20, 792, 20, 2, 457, 793, 1307, 570, 117, 2, 1308, 1309, 44, 247, 3, 149, 1310, 16, 1311, 16, 794, 5, 795, 97, 201, 2, 458, 1312, 248, 1313, 117, 796, 459, 1]]\n",
      "bere a german asraafo faa belgium hye so duu france pɛ na yefii ase huu sɛ wɔde nasifo frankaa asisi asɔredan ahorow atifi ɛwom mpo sɛ na france frankaa da so ara si kuropɔn mu asa a amanfo hyia wɔ hɔ no so de . <end>\n",
      "\n",
      "The French had closed our Kingdom Hall and banned the work of Jehovah s Witnesses and we were already working underground when the Germans came .\n",
      "[[2, 458, 24, 797, 67, 319, 459, 6, 798, 2, 90, 5, 41, 17, 79, 6, 44, 55, 1314, 282, 367, 28, 2, 799, 98, 1]]\n",
      "na france aban ato yɛn ahenni asa mu abara yehowa adansefo adwuma wɔ sum ase wɔ sum ase wɔ adelaide ! <end>\n",
      "\n",
      "But the effort to crush the Witnesses intensified .\n",
      "[[36, 2, 1315, 3, 1316, 2, 79, 1317, 1]]\n",
      "na mmɔden a wɔbɔe sɛ wobegu yɛn adwuma bi yɛɛ den . <end>\n",
      "\n",
      "Two years later at age I was baptized .\n",
      "[[150, 126, 157, 22, 78, 7, 10, 579, 1]]\n",
      "wɔbɔɔ me asu mfe abien akyi bere a na madi mfe . <end>\n",
      "\n",
      "One month later September at two o clock in the afternoon the doorbell rang .\n",
      "[[50, 283, 157, 1318, 22, 150, 781, 446, 8, 2, 580, 2, 1319, 1320, 1]]\n",
      "wɔ ɔsram biako akyi september awia bere nnɔn abien no yɛn fie pon ano dɔn no bɔe . <end>\n",
      "\n",
      "Dad was due home from work .\n",
      "[[244, 10, 1321, 118, 32, 90, 1]]\n",
      "na bere a papa fie fie . <end>\n",
      "\n",
      "I jumped up opened the door and ran into his arms .\n",
      "[[7, 1322, 56, 800, 2, 249, 6, 368, 87, 43, 369, 1]]\n",
      "mesɔre kobuee pon no yɛɛ ɔbarima no atuu . <end>\n",
      "\n",
      "A man behind him shouted Heil Hitler ! \n",
      "[[4, 131, 460, 85, 801, 215, 202, 39]]\n",
      "ɔbarima bi a ogyina n’akyi teɛɛm bere a ! <end>\n",
      "\n",
      "Down on my feet again I then realized that the man I had embraced was an SS soldier !\n",
      "[[216, 16, 15, 313, 250, 7, 77, 370, 9, 2, 131, 7, 24, 1323, 10, 59, 581, 1324, 39]]\n",
      "hitler mo ! migyaee ne mu ɛhɔ na ɔbarima a meyɛɛ no atuu no atuu no yɛ ss sraani bi ! <end>\n",
      "\n",
      "They sent me off to my room and gave my mother a four hour cross examination .\n",
      "[[30, 320, 19, 284, 3, 15, 316, 6, 182, 15, 102, 4, 461, 1325, 321, 802, 1]]\n",
      "wɔde me kɔɔ asennibea hɔ ansa na wɔde nnɔhwerew anan kobisabisaa me maame nsɛm . <end>\n",
      "\n",
      "As they left one of them shouted You won t see your husband anymore !\n",
      "[[20, 30, 217, 50, 5, 46, 801, 18, 803, 51, 149, 47, 582, 1326, 39]]\n",
      "bere a wɔrefi hɔ akɔ no wɔn mu biako teɛɛm sɛ worenhu wo kunu bio ! <end>\n",
      "\n",
      "You and your child will go the same way ! \n",
      "[[18, 6, 47, 80, 61, 101, 2, 218, 89, 39]]\n",
      "yɛbɛyɛ wo ne wo ba no nso saa ara ! <end>\n",
      "\n",
      "Dad had been arrested that morning .\n",
      "[[244, 24, 81, 462, 9, 145, 1]]\n",
      "na m’ani gye hɔ a eye gye hɔ yɛ den nso ho . <end>\n",
      "\n",
      "He had had his monthly salary in his pocket .\n",
      "[[21, 24, 24, 43, 1327, 1328, 8, 43, 804, 1]]\n",
      "na wagye ne bosome akatua ma ɛwɔ ne kotoku mu . <end>\n",
      "\n",
      "The SS closed down the bank account and refused my mother a working card a necessary document to get a job .\n",
      "[[2, 581, 797, 216, 2, 1329, 805, 6, 371, 15, 102, 4, 282, 806, 4, 807, 1330, 3, 70, 4, 251, 1]]\n",
      "ss asraafo no de wɔn nsa too me papa sika a ɛda sika korabea so na wɔamma me maame adwuma wɔ sukuu no nyinaa guu me maame ! <end>\n",
      "\n",
      "Their policy now was No means of living for those vermin ! \n",
      "[[35, 808, 128, 10, 66, 463, 5, 252, 12, 115, 1331, 39]]\n",
      "afei na wɔn mmara ne sɛ momfa asetra mu ahiade nkame saa nkurɔfo a wɔn ho yɛ abufuw no ! <end>\n",
      "\n",
      "Persecution at School\n",
      "[[1332, 22, 73]]\n",
      "sukuu ɔtae <end>\n",
      "\n",
      "During this time the pressures at the college preparatory school I was attending continued to increase .\n",
      "[[144, 33, 52, 2, 809, 22, 2, 1333, 1334, 73, 7, 10, 1335, 583, 3, 1336, 1]]\n",
      "ɔhaw a na ɛwɔ sukuu no mu kɔɔ so yɛɛ den saa bere nyinaa . <end>\n",
      "\n",
      "Whenever the teacher came to class all students had to stand with outstretched arms and say Heil Hitler . \n",
      "[[584, 2, 585, 98, 3, 168, 37, 448, 24, 3, 464, 14, 810, 369, 6, 125, 215, 202, 1]]\n",
      "bere nyinaa mu no ka kyerɛɛ yɛn sɛ ɛsɛ sɛ hitler no bere nyinaa dodow a wɔda ho wiei no na mede akode bedi dwuma yiye . <end>\n",
      "\n",
      "When the priest came for religious education he would come in and say Heil Hitler blessed is the one that cometh in the name of the Lord . \n",
      "[[28, 2, 782, 98, 12, 199, 1337, 21, 72, 169, 8, 6, 125, 215, 202, 1338, 11, 2, 50, 9, 1339, 8, 2, 372, 5, 2, 586, 1]]\n",
      "sɛ nhwɛso no abeawa kɔɔ adelaide na adwumaden no nyɛ sɛ akyiri yi bɛma me mfɛfo sukuufo no mede nhoma yi ma no mu yiye . <end>\n",
      "\n",
      "The class would answer Heil Hitler Amen ! \n",
      "[[2, 168, 72, 373, 215, 202, 1340, 39]]\n",
      "na sukuufo no gye so nso wɔ sukuu dan mu nso ! <end>\n",
      "\n",
      "I refused to say Heil Hitler and word of this came before the school director .\n",
      "[[7, 371, 3, 125, 215, 202, 6, 374, 5, 33, 98, 111, 2, 73, 285, 1]]\n",
      "manka hitler mo no bi na ɛho asɛm koduu sukuu panyin no anim . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, test_sent in enumerate(raw_data_en[:50]):\n",
    "    test_sequence = normalize_eng(test_sent)\n",
    "    predict(test_sequence)\n",
    "    # print(÷)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvFSXTMVCdWR"
   },
   "outputs": [],
   "source": [
    "test_sents = (\n",
    "\n",
    "   'What kind of wrong activities are common where you live, and how are you and your family affected?',\n",
    "    'How are you benefiting from an evening set aside for family worship or personal study?',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'He asked the graduating class: “How are you going to view yourselves as you go to your missionary assignment?',\n",
    "    \"How Are You Benefiting From the New Meeting Format and the Workbook?: (5 min.) Discussion.\",\n",
    "    \"Just how are you recommending yourself to others in this regard?\",\n",
    "    \"“Serpents, offspring of vipers,” he says, “how are you to flee from the judgment of Gehenna?”\",\n",
    "    \"Ask, ‘How are you?’\",\n",
    "    \"How Are You Affected by God’s Dignity and Splendor?\",\n",
    "    \"By the way, how are you doing on time?\",\n",
    "    \"How Are You Affected?\",\n",
    "    \"How are you treating the gift that God has given you?\",\n",
    "    \"How are you involved in the issue that Satan raised regarding Job?\",\n",
    "    \"How are you personally affected?\",\n",
    "    \"All the information needed to repeat a phrase like “How are you doing?”\",\n",
    "    \"Ask, ‘How are you?’\",\n",
    "    'What a ridiculous concept!',\n",
    "    'Your idea is not entirely crazy.',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'What he did is very wrong.',\n",
    "    \"All three of you need to do that.\",\n",
    "    \"Are you giving me another chance?\",\n",
    "    \"Both Tom and Mary work as models.\",\n",
    "    \"Can I have a few minutes, please?\",\n",
    "    \"Could you close the door, please?\",\n",
    "    \"Did you plant pumpkins this year?\",\n",
    "    \"Do you ever study in the library?\",\n",
    "    \"Don't be deceived by appearances.\",\n",
    "    \"Excuse me. Can you speak English?\",\n",
    "    \"Few people know the true meaning.\",\n",
    "    \"Germany produced many scientists.\",\n",
    "    \"Guess whose birthday it is today.\",\n",
    "    \"He acted like he owned the place.\",\n",
    "    \"Honesty will pay in the long run.\",\n",
    "    \"How do we know this isn't a trap?\",\n",
    "    \"I can't believe you're giving up.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xtMCIhk0ERJ3",
    "outputId": "e5ca3afd-667e-4185-9987-34d9cc310ed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What kind of wrong activities are common where you live and how are you and your family affected ?\n",
      "[[62, 1437, 5, 426, 1916, 25, 743, 133, 18, 344, 6, 54, 25, 18, 6, 47, 75, 34]]\n",
      "wɔ wɔn nkwa nna tiaa yɛn a ɛwɔ buronya ho nsɛm taa afahyɛ a ɛwɔ afahyɛ a ɛwɔ nhwiren a ɛwɔ owia no mu ? <end>\n",
      "\n",
      "How are you benefiting from an evening set aside for family worship or personal study ?\n",
      "[[54, 25, 18, 32, 59, 866, 338, 2319, 12, 75, 197, 27, 1301, 1261, 34]]\n",
      "mmofra a wofi saa mu ahoɔden pɛ a wɔfrɛ no monarch pii wɔ mfinimfini . <end>\n",
      "\n",
      "A man s worth lies in what he is .\n",
      "[[4, 131, 17, 1071, 8, 62, 21, 11, 1]]\n",
      "eyi nkyerɛ sɛ ɛsɛ sɛ woma w’awofo a wɔanyinyin no . <end>\n",
      "\n",
      "He asked the graduating class How are you going to view yourselves as you go to your missionary assignment ?\n",
      "[[21, 208, 2, 168, 54, 25, 18, 213, 3, 731, 20, 18, 101, 3, 47, 34]]\n",
      "so ɔbɔɔ mmɔden sɛ meyɛ yehowa adansefo ani gye ho ? <end>\n",
      "\n",
      "How Are You Benefiting From the New Meeting Format and the Workbook ? min . Discussion .\n",
      "[[54, 25, 18, 32, 2, 189, 6, 2, 34, 1, 573, 1]]\n",
      "ɛbɛyɛ sɛ afahyɛ no afahyɛ foforo a ɛde awufo ansa na wɔka biribi afahyɛ no afahyɛ a wɔde wom na ɛhyerɛn bɛyɛ . <end>\n",
      "\n",
      "Just how are you recommending yourself to others in this regard ?\n",
      "[[156, 54, 25, 18, 453, 3, 185, 8, 33, 34]]\n",
      "dɛn na nnipa bɛyɛ ayɛ di dwuma ho to dɛn ahu dɛn so ? <end>\n",
      "\n",
      " Serpents offspring of vipers he says how are you to flee from the judgment of Gehenna ? \n",
      "[[5, 21, 121, 54, 25, 18, 3, 32, 2, 1390, 5, 34]]\n",
      "ɔde fa bɛyɛ ho ba no fa ? <end>\n",
      "\n",
      "Ask How are you ? \n",
      "[[226, 54, 25, 18, 34]]\n",
      "dɛn na wubetumi ayɛ anaa ba ba asɛm ? <end>\n",
      "\n",
      "How Are You Affected by God s Dignity and Splendor ?\n",
      "[[54, 25, 18, 26, 64, 17, 6, 34]]\n",
      "ɔwofo a wubetumi nyamesom di ? <end>\n",
      "\n",
      "By the way how are you doing on time ?\n",
      "[[26, 2, 89, 54, 25, 18, 549, 16, 52, 34]]\n",
      "so nyamesom abusua a ɔwofo yi ba ho so nyamesom mu nyamesom ho ? <end>\n",
      "\n",
      "How Are You Affected ?\n",
      "[[54, 25, 18, 34]]\n",
      "so dɛn honam bi ? <end>\n",
      "\n",
      "How are you treating the gift that God has given you ?\n",
      "[[54, 25, 18, 2, 206, 9, 64, 93, 288, 18, 34]]\n",
      "dɛn na ɛsɛ sɛ onyankopɔn wo mma yɛ den a wɔyɛ onyankopɔn hɔ apontow ahorow no ? <end>\n",
      "\n",
      "How are you involved in the issue that Satan raised regarding Job ?\n",
      "[[54, 25, 18, 1011, 8, 2, 698, 9, 554, 832, 251, 34]]\n",
      "wɔ mmeae a ɛte saa fi mmeae a ɔwofo no mu hu ? <end>\n",
      "\n",
      "How are you personally affected ?\n",
      "[[54, 25, 18, 2963, 34]]\n",
      "dɛn na ɛwɔ saa hu ne nneɛma a ? <end>\n",
      "\n",
      "All the information needed to repeat a phrase like How are you doing ? \n",
      "[[37, 2, 689, 493, 3, 4, 2278, 92, 54, 25, 18, 549, 34]]\n",
      "ɔde n’ani wo ho adwuma pa a wɔyɛ adwuma yi ho atosɛm no so kosi bere a ? <end>\n",
      "\n",
      "Ask How are you ? \n",
      "[[226, 54, 25, 18, 34]]\n",
      "dɛn na wubetumi ayɛ anaa ba ba asɛm ? <end>\n",
      "\n",
      "What a ridiculous concept !\n",
      "[[62, 4, 39]]\n",
      "na sɛ ɛba ! <end>\n",
      "\n",
      "Your idea is not entirely crazy .\n",
      "[[47, 2429, 11, 23, 2954, 1]]\n",
      "ɛtɔ wo wɔ afahyɛ a ɛwɔ hɔ . <end>\n",
      "\n",
      "A man s worth lies in what he is .\n",
      "[[4, 131, 17, 1071, 8, 62, 21, 11, 1]]\n",
      "eyi nkyerɛ sɛ ɛsɛ sɛ woma w’awofo a wɔanyinyin no . <end>\n",
      "\n",
      "What he did is very wrong .\n",
      "[[62, 21, 227, 11, 200, 426, 1]]\n",
      "ɛyɛ yehowa mmoa a ɛho den no betumi ayɛ biribi <end>\n",
      "\n",
      "All three of you need to do that .\n",
      "[[37, 279, 5, 18, 266, 3, 40, 9, 1]]\n",
      "ɛnyɛ nwonwa sɛ wobehu ɔfoforo so yɛn anigye . <end>\n",
      "\n",
      "Are you giving me another chance ?\n",
      "[[25, 18, 135, 19, 158, 1459, 34]]\n",
      "so adekyɛ buronya ho su hɔ yɛ adapɛn kakraa bi ? <end>\n",
      "\n",
      "Both Tom and Mary work as models .\n",
      "[[670, 6, 90, 20, 1]]\n",
      "ɛho nhwiren kɔ mu <end>\n",
      "\n",
      "Can I have a few minutes please ?\n",
      "[[58, 7, 29, 4, 300, 816, 1085, 34]]\n",
      "mama so ? <end>\n",
      "\n",
      "Could you close the door please ?\n",
      "[[170, 18, 1155, 2, 249, 1085, 34]]\n",
      "so woadi bankye pɛn ? <end>\n",
      "\n",
      "Did you plant pumpkins this year ?\n",
      "[[227, 18, 1826, 33, 94, 34]]\n",
      "so onyankopɔn penee nkabom yi so ? <end>\n",
      "\n",
      "Do you ever study in the library ?\n",
      "[[40, 18, 393, 1261, 8, 2, 34]]\n",
      "wofi saa ase anɔpa yi ase hɔ anɔpa . <end>\n",
      "\n",
      "Don t be deceived by appearances .\n",
      "[[193, 51, 31, 1661, 26, 1]]\n",
      "mfofantɔ taa ne wɔn ankasa . <end>\n",
      "\n",
      "Excuse me . Can you speak English ?\n",
      "[[19, 1, 58, 18, 34]]\n",
      "so adekyɛ ho ɔdenimfo bi ? <end>\n",
      "\n",
      "Few people know the true meaning .\n",
      "[[300, 88, 134, 2, 640, 1]]\n",
      "ɛnsɛ sɛ wɔde di afahyɛ a ɛwɔ owia . <end>\n",
      "\n",
      "Germany produced many scientists .\n",
      "[[2066, 153, 1]]\n",
      "j . <end>\n",
      "\n",
      "Guess whose birthday it is today .\n",
      "[[481, 1157, 13, 11, 703, 1]]\n",
      "ɛyɛ amanne titiriw anɔpa titiriw . <end>\n",
      "\n",
      "He acted like he owned the place .\n",
      "[[21, 92, 21, 2, 600, 1]]\n",
      "na mmofra no ho pii pɛ <end>\n",
      "\n",
      "Honesty will pay in the long run .\n",
      "[[2155, 61, 2117, 8, 2, 229, 2936, 1]]\n",
      "na papa wɔ ase hɔ nyinaa . <end>\n",
      "\n",
      "How do we know this isn t a trap ?\n",
      "[[54, 40, 44, 134, 33, 302, 51, 4, 34]]\n",
      "so dɛn ankasa buronya ho ? <end>\n",
      "\n",
      "I can t believe you re giving up .\n",
      "[[7, 58, 51, 355, 18, 745, 135, 56, 1]]\n",
      "etumi yɛ yiye wɔ hɔ a nsu tɔ kɛse . <end>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, test_sent in enumerate(test_sents):\n",
    "    test_sequence = normalize_eng(test_sent)\n",
    "    predict(test_sequence)\n",
    "    # print(÷)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkZZTKI6FDOh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "eng_twi_transformer_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
